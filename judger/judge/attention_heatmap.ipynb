{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, PreTrainedModel\n",
    "from generate_answers_lit import TEMPLATES\n",
    "\n",
    "TOKENIZER_PATH = Path(\"~/Llama-2-7b-hf/\")\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MODEL_NAME = None\n",
    "MODEL_PATH = Path(\"~/Llama-2-7b-rag-baseline/epoch=4-step=120.ckpt/\")\n",
    "PROMPT_STYLE: TEMPLATES = 'chatml' # 'chatml', 'chatml_enhanced', 'llama-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "model_name_or_path = MODEL_NAME if MODEL_NAME else MODEL_PATH\n",
    "\n",
    "# Process outfile name\n",
    "if isinstance(model_name_or_path, Path):\n",
    "    outfile_prefix = str(model_name_or_path).lstrip('~/')\n",
    "else:\n",
    "    assert isinstance(model_name_or_path, str)\n",
    "    outfile_prefix = model_name_or_path\n",
    "\n",
    "outfile_prefix = outfile_prefix.replace('/', '_')\n",
    "outfile_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix invalid path issues\n",
    "TOKENIZER_PATH = TOKENIZER_PATH.expanduser()\n",
    "if isinstance(model_name_or_path, Path):\n",
    "    model_name_or_path = model_name_or_path.expanduser()\n",
    "\n",
    "# Ensure the correct paths are loaded\n",
    "print(\"Tokenizer path:\", TOKENIZER_PATH)\n",
    "print(\"Model path:\", model_name_or_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    TOKENIZER_PATH,\n",
    "    device_map='cuda'\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# Load the model\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='cuda',\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    \"\"\"Utility function to help prevent OOM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of prompts\n",
    "from generate_answers_lit import get_prompt\n",
    "from load_jsonl_files import load_jsonl_files\n",
    "\n",
    "# Load jsonl files\n",
    "dataset_list = load_jsonl_files('dataset')\n",
    "# Get list of prompts\n",
    "prompts = []\n",
    "for datarow in dataset_list:\n",
    "    prompt = get_prompt(datarow, PROMPT_STYLE)\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "matplotlib.use('Agg')\n",
    "import seaborn\n",
    "import pandas\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def pad_tensor(tensor: torch.Tensor, shape: Tuple) -> torch.Tensor:\n",
    "    \"\"\"Pad tensor with zeroes to specified shape.\"\"\"\n",
    "    result = torch.zeros(*shape)\n",
    "    result[:tensor.shape[0], :tensor.shape[1], :tensor.shape[2]] = tensor\n",
    "    return result\n",
    "\n",
    "def merge_tensors(tensors: Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Merge tuple of tensors (with varying dimensions) into a single tensor.\"\"\"\n",
    "    num_answer_weights = sum(tensor.shape[1] for tensor in tensors[1:])\n",
    "    print(f\"num_answer_weights: {num_answer_weights}\")\n",
    "    list_of_tensors = list(tensors)\n",
    "    for i in range(len(tensors)-1, -1, -1):\n",
    "        list_of_tensors[i:i+1] = torch.chunk(tensors[i], tensors[i].shape[1], dim=1)\n",
    "    shape = list_of_tensors[-1].shape\n",
    "    each_tensor_shape = (shape[0], 1, shape[2])\n",
    "    padded_tensors = [pad_tensor(tensor, each_tensor_shape) for tensor in list_of_tensors]\n",
    "    return torch.cat(padded_tensors, dim=1)\n",
    "\n",
    "def get_tokens_and_att_weights(model: PreTrainedModel, prompt: str) -> None:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').cuda()\n",
    "\n",
    "    # Generate attention weights\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=4096,\n",
    "        pad_token_id=0,\n",
    "        do_sample=False,\n",
    "        # temperature=0.01,\n",
    "        # top_p=None,\n",
    "        # top_k=200,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    # TODO: Modify huggingface transformers code to save only last-layer attn weights\n",
    "    # instead of all 32 layers' weights. This will help with CUDA OOM and greatly increase performance.\n",
    "    attention_sequences = output.attentions\n",
    "    for attention in attention_sequences:\n",
    "        assert len(attention) == 32\n",
    "\n",
    "    tuple_of_last_attention_layers = tuple(attention[-1].squeeze(dim=0) for attention in attention_sequences)\n",
    "    last_layer_attentions = merge_tensors(tuple_of_last_attention_layers)\n",
    "\n",
    "    sequence_ids = output.sequences[0]\n",
    "    assert sequence_ids[:len(input_ids[0])].equal(input_ids[0]), \\\n",
    "        \"The prompt is not in the prefix of the model output.\"\n",
    "    num_answer_ids = len(sequence_ids) - len(input_ids[0])\n",
    "    print(f\"num_answer_ids: {num_answer_ids}\")\n",
    "    # # Decode the generated answer\n",
    "    # answer = tokenizer.decode(sequence_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    # print(answer)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sequence_ids, skip_special_tokens=False)\n",
    "\n",
    "    # Average attention weights across all heads\n",
    "    mean_attention_weights = last_layer_attentions.mean(dim=0).detach().cpu()\n",
    "    print(f\"mean_attention_weights.shape: {mean_attention_weights.shape}\")\n",
    "    tokens = tokens[:-1]\n",
    "    return tokens, mean_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(\n",
    "    tokens: torch.Tensor,\n",
    "    mean_attention_weights: torch.Tensor,\n",
    "    cmap: str = 'Reds',\n",
    "    title: Optional[str] = None,\n",
    "    outpath: Path = Path('./heatmaps/heatmap.png'),\n",
    ") -> None:\n",
    "    if outpath.exists():\n",
    "        print(f\"{outpath} already exists. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {outpath}...\")\n",
    "    assert len(tokens) == len(mean_attention_weights), \\\n",
    "        f\"tokens: {len(tokens)}, weights: {len(mean_attention_weights)}\"\n",
    "\n",
    "    start = 0\n",
    "    seq_len = len(tokens)\n",
    "    figsize = (192, 192)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pandas.DataFrame(\n",
    "        mean_attention_weights[start:start+seq_len, start:start+seq_len],\n",
    "        index=tokens[start:start+seq_len],\n",
    "        columns=tokens[start:start+seq_len]\n",
    "    )\n",
    "\n",
    "    print(\"Plotting heatmap...\")\n",
    "    # Plot the heatmap\n",
    "    matplotlib.pyplot.figure(figsize=figsize)\n",
    "    if cmap == 'hot':\n",
    "        cmap = matplotlib.colormaps['hot'].copy()\n",
    "        cmap.set_bad(color='black')\n",
    "    hm = seaborn.heatmap(\n",
    "        df,\n",
    "        annot=False,\n",
    "        cmap=cmap,\n",
    "        cbar=False,\n",
    "        square=True,\n",
    "        norm=LogNorm(),\n",
    "        xticklabels=1,\n",
    "        yticklabels=1,\n",
    "    )\n",
    "    if title:\n",
    "        hm.set_title(title)\n",
    "    hm.set_xticklabels(hm.get_xmajorticklabels(), fontsize=7)\n",
    "    hm.set_yticklabels(hm.get_ymajorticklabels(), fontsize=7)\n",
    "    matplotlib.pyplot.tick_params(\n",
    "        axis='x',\n",
    "        which='both',\n",
    "        bottom=True,\n",
    "        top=False,\n",
    "        labelbottom=True,\n",
    "        labeltop=False,\n",
    "        rotation=90\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "    matplotlib.pyplot.savefig(outpath, bbox_inches='tight')\n",
    "    print(f\"Heatmap saved to {outpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "clear_cache()\n",
    "pool = multiprocessing.Pool(\n",
    "    processes=multiprocessing.cpu_count(),\n",
    "    maxtasksperchild=8, # Required to prevent memory leak; adjust based on available RAM\n",
    ")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    outfolder = Path('./heatmaps/')\n",
    "    outfile = Path(outfile_prefix + '_' + str(i) + '.png')\n",
    "    outpath = outfolder / outfile\n",
    "    if outpath.exists():\n",
    "        print(f\"{outpath} already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        tokens, mean_attention_weights = get_tokens_and_att_weights(model, prompt)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(f\"ERROR: CUDA OOMed. Skipping {outpath}.\")\n",
    "        clear_cache()\n",
    "        continue\n",
    "\n",
    "    clear_cache()\n",
    "    # Generate heatmaps for all prompts\n",
    "    pool.apply_async(\n",
    "        plot_attention_heatmap,\n",
    "        kwds = {\n",
    "            'tokens': tokens,\n",
    "            'mean_attention_weights': mean_attention_weights,\n",
    "            # cmap='hot',\n",
    "            'outpath': outpath\n",
    "        }\n",
    "    )\n",
    "    clear_cache()\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
